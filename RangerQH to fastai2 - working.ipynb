{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A first draft at porting @lessw2020's RangerQH optimizer from fastai v1 to fastai v2\n",
    "- MNIST code taken from the excellent torch.nn tutorial, [What is torach.NN Reall?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n",
    "- @lessw2020's RangerQH code here: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/rangerqh.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision feather-format kornia pyarrow --upgrade   > /dev/null\n",
    "# !pip install git+https://github.com/fastai/fastai_dev                    > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fastai2.basics           import *\n",
    "from fastai2.vision.all       import *\n",
    "from fastai2.medical.imaging  import *\n",
    "from fastai2.callback.tracker import *\n",
    "from fastai2.callback.all     import *\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "from torch import nn, optim\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)\n",
    "    \n",
    "# class Mnist_CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#     def forward(self, xb):\n",
    "#         xb = xb.view(-1, 1, 28, 28)\n",
    "#         xb = F.relu(self.conv1(xb))\n",
    "#         xb = F.relu(self.conv2(xb))\n",
    "#         xb = F.relu(self.conv3(xb))\n",
    "#         xb = F.avg_pool2d(xb, 4)\n",
    "#         return xb.view(-1, xb.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RangerQH v2 attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai2.basics import Optimizer as Optimizerv2\n",
    "\n",
    "def RangerQH_v2(params, lr=1e-3, mom=0.9, sqr_mom=0.99, \n",
    "                 eps=1e-8, wd=0., k=6, alpha=.5, decouple_wd=False, betas=(0.9, 0.999)): # betas, nus\n",
    "    from functools  import partial\n",
    "    steppers = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    steppers.append(partial(rangerqh_step, nus=(.7, 1.0)))\n",
    "    stats = [average_grad, step_stat, partial(betas_stat,betas=betas), d_p_grad, exp_avg_stat, slow_buffer_stat]\n",
    "    return Optimizer(params, steppers, stats=stats, lr=lr, k=k, alpha=alpha,  # betas=betas, nus=nus\n",
    "                     mom=mom, sqr_mom=sqr_mom, \n",
    "                     #grad_avg=grad_avg, #sqr_avg=sqr_avg, \n",
    "                     eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betas_stat(state, p, betas, **kwargs):\n",
    "    beta1, beta2 = betas[0], betas[1]\n",
    "    if 'beta1_weight' not in state: state['beta1_weight'] = 0.0\n",
    "    state['beta1_weight'] = 1.0 + (beta1 * state['beta1_weight'])\n",
    "    \n",
    "    if 'beta2_weight' not in state: state['beta2_weight'] = 0.0\n",
    "    state['beta2_weight'] = 1.0 + (beta2 * state['beta2_weight'])  \n",
    "    return state\n",
    "\n",
    "def exp_avg_stat(state, p, **kwargs):\n",
    "    # EXP AVG THINGS\n",
    "    beta1_adj = 1.0 - (1.0 / state['beta1_weight'])\n",
    "    beta2_adj = 1.0 - (1.0 / state['beta2_weight'])\n",
    "\n",
    "    if 'exp_avg' not in state: state['exp_avg'] = torch.zeros_like(p.data)\n",
    "    if 'exp_avg_sq' not in state: state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "    state['exp_avg'].mul_(beta1_adj).add_(1.0 - beta1_adj, state['d_p'])\n",
    "    state['exp_avg_sq'].mul_(beta2_adj).add_(1.0 - beta2_adj, state['d_p_sq'])\n",
    "    return state\n",
    "\n",
    "def d_p_grad(state, p,  **kwargs):\n",
    "    state['d_p'] = p.grad.data\n",
    "    state['d_p_sq'] = state['d_p'].mul(state['d_p'])\n",
    "    return state\n",
    "\n",
    "def slow_buffer_stat(state, p, **kwargs):\n",
    "    if 'slow_buffer' not in state: \n",
    "        state['slow_buffer'] =  torch.empty_like(p.data)\n",
    "        state['slow_buffer'].copy_(p.data)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rangerqh_step(p, lr, mom, sqr_mom, step, \n",
    "                  beta1_weight, beta2_weight, exp_avg, exp_avg_sq, slow_buffer, grad_avg, d_p, d_p_sq, nus,\n",
    "                  eps, wd, k, alpha, **kwargs): # betas, nus, grad_avg, sqr_avg, \n",
    "        \"\"\"\n",
    "            Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        #nus=(.7, 1.0)\n",
    "        nu_1 = nus[0]\n",
    "        nu_2 = nus[1]\n",
    "        \n",
    "        if d_p.is_sparse:\n",
    "            raise RuntimeError(\"QHAdam does not support sparse gradients\")\n",
    "            \n",
    "        avg_grad = exp_avg.mul(nu_1)\n",
    "        if nu_1 != 1.0:\n",
    "            avg_grad.add_(1.0 - nu_1, d_p)\n",
    "\n",
    "        avg_grad_rms = exp_avg_sq.mul(nu_2)\n",
    "        if nu_2 != 1.0:\n",
    "            avg_grad_rms.add_(1.0 - nu_2, d_p_sq)\n",
    "            \n",
    "        avg_grad_rms.sqrt_()\n",
    "        if eps != 0.0:\n",
    "            avg_grad_rms.add_(eps)\n",
    "        \n",
    "        p.data.addcdiv_(-lr, avg_grad, avg_grad_rms)\n",
    "        \n",
    "        # LOOKAHEAD STEPPER\n",
    "        #integrated look ahead...\n",
    "        #if param_state['step'] % self.k ==0: #group['k'] == 0:\n",
    "        if step % k ==0: \n",
    "            #slow_p = param_state['slow_buffer'] #get access to slow param tensor\n",
    "            slow_p = slow_buffer #get access to slow param tensor\n",
    "            # CACLC\n",
    "            slow_p.add_(alpha, p.data - slow_p)  #(fast weights - slow weights) * alpha\n",
    "            # RETURN P\n",
    "            p.data.copy_(slow_p)  # copy interpolated weights to RAdam param tensor\n",
    "                \n",
    "        #return loss\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "def get_sgd_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "def get_ramgerqhv2_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, RangerQH_v2(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Accuracy : 0.6990000009536743\n",
      "RangerQH_v2 Accuracy : 0.7013000249862671\n"
     ]
    }
   ],
   "source": [
    "loss_func = nll\n",
    "lr = 0.5   # learning rate\n",
    "epochs = 50  # how many epochs to train for\n",
    "bs = 128\n",
    "\n",
    "opt_nms = ['SGD', 'RangerQH_v2']\n",
    "\n",
    "# Initialise models\n",
    "sgd_model, sgd_opt = get_sgd_model()\n",
    "rangerqh_model, rangerqh_opt = get_ramgerqhv2_model()\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = sgd_model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        loss.backward()\n",
    "        sgd_opt.step()\n",
    "        sgd_opt.zero_grad()\n",
    "    \n",
    "print(f'{opt_nms[0]} Accuracy : {accuracy(sgd_model(x_valid), y_valid)}')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = rangerqh_model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        loss.backward()\n",
    "        rangerqh_opt.step()\n",
    "        rangerqh_opt.zero_grad()\n",
    "    \n",
    "print(f'{opt_nms[1]} Accuracy : {accuracy(rangerqh_model(x_valid), y_valid)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
